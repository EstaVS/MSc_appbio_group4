---
title: "SharedRMarkdown"
output: html_document
date: "2024-11-27"
---

Outline:

This R Markdown script provides the code needed to reproduce the RNA-sequencing analyses conducted in this study. The script below can be utilised to replicate the data and analysis used in this work.

# Initial setup of the Work environment and Data:

1. Initially, log into the CREATE note and navigated to scratch place

```{bash}

srun -p msc_appbio --pty /bin/bash

/scratch_tmp/grp/msc_appbio/group4_tmp/ #navigating to the scratch place
```

2. Preparing the work space 

Create new directories to store files within project 

```{bash}

mkdir raw_data_rna
mkdir processed_data 
mkdir ref_genome

```
# Downloading RNA-Seq fastq files 

1. Create a nano script containing all the URLs for the fastq files 

```{bash}
nano links2download.txt 
```

2. Writing the download script

Bash script (download.sh) was created to automate the download process using the weget command:

```{bash}
nano download.sh #creates nano script

#In the nano script 

#!/bin/bash
#SBATCH --job-name=download_job      # Job name
#SBATCH --output=download_job_%j.log # Output log file (%j will be replaced with job ID)
#SBATCH --error=download_job_%j.err  # Error log file (%j will be replaced with job ID)
#SBATCH --ntasks=1                   # Number of tasks (processes)
#SBATCH --time=00:30:00              # Time limit (hh:mm:ss)
#SBATCH --mem=1G                     # Memory per node (adjust as needed)
#SBATCH --partition=standard         # Partition to submit to (adjust to your cluster setup)

# Load wget
module load wget

# Text file containing URLs
LINK_FILE="/scratch_tmp/grp/msc_appbio/practice/links2download.txt"

# Check if the file exists
if [[ ! -f "$LINK_FILE" ]]; then
echo "Error: $LINK_FILE does not exist."
exit 1
fi

# Create a directory to store the downloaded files
DOWNLOAD_DIR="/scratch_tmp/grp/msc_appbio/practice/downloads"
mkdir -p "$DOWNLOAD_DIR"

# Read each line of URL from the file and download it
while IFS= read -r URL; do
if [[ -n "$URL" ]]; then
echo "Downloading: $URL"
wget -P "$DOWNLOAD_DIR" "$URL" || echo "Failed to download: $URL"
fi
done < "$LINK_FILE"

echo "Download complete. Files saved in '$DOWNLOAD_DIR'."

```

3. Submitting the job 

The download.sh script was then submitted to SLURM 

```{bash}
sbatch -p msc_appbio download.sh /bin/bash
```
The output should include a list of fastq.gz files from the RNA-seq data in the downloads directory. 

# Installing the tools needed for our environment using Conda

1. Create nano script to create a Conda environment with specific versions specified in the paper

```{bash}
nano environment.sh

# In the nano script 

#!/bin/bash

echo "start of the pipeline"

# Add conda channels
conda config --add channels bioconda
conda config --add channels conda-forge

# Create a new conda environment with specific versions
conda create -n bioenv cutadapt=2.3 faqcs=2.08 star= 2.5.2a fdrtool= 1.2.15 -y

#Load Fastqc

module load fastqc

# Print a message about how to activate the environment
echo "To activate your environment, run: conda activate bioenv" 


echo "end of the pipeline"

Then we need to activate it using 
./environment.sh
But permission is denied
conda activate bioenv


```

2. Activating the Conda environment 

Ensures that the Conda environment is added to the shell environment so that the specified tools and versions in bioenv can be used.

```{bash}
./environment.sh 
conda activate bioenv
```

# Quality Control of our files, using fastqc

1. Create a Nano script to fastqc the fastq.gz files 

```{bash}
nano fastqc_RNA

# In the nano script

#!/bin/bash

echo "start of the pipeline"

#Load fastqc - if not loaded previously 
module load fastqc 

# Define base and results directories

baseDirectory="/scratch_tmp/grp/msc_appbio/group4_tmp/raw_data_rna/fastq"
resultsDirectory="/scratch/grp/msc_appbio/group4_tmp/processed_data/"

# Create the results directory 
mkdir -p "$resultsDirectory"

# Run Fastqc on all .fastq.gz files in baseDirectory
fastqc -o "$resultsDirectory" -t 4 "$baseDirectory"/*.fastq.gz 
#(-o represents the output directory)

echo "end of the pipeline"

```

If the fastqc.html files look good i.e. have good quality reads and do not require adapter trimming, we can proceed with the raw .fastq files without further processing. 

# Aligning Trimmed reads to the reference genome of S. cerevisiae CEN.PK113â€7D

1. Obtaining the reference genome

```{bash}
wget -c -i https://ftp.ensemblgenomes.ebi.ac.uk/pub/fungi/release-60/fasta/fungi_ascomycota1_collection/saccharomyces_cerevisiae_cen_pk113_7d_gca_000269885/dna/Saccharomyces_cerevisiae_cen_pk113_7d_gca_000269885.ASM26988v1.dna.toplevel.fa.gz 

#should be directed to the ref_genome directory

```

2. Alignment with STAR 









Rough notes from 26th Nov:

For git:
ssh-keygen -t ed25519 -C "eraaziz13@gmail.com"

cat /users/k24088153/.ssh/id_ed25519.pub

git config --global --add safe.directory /scratch_tmp/grp/msc_appbio/group4_tmp/MSc_appbio_group4
git status 
This was so that we could all access our git repository. 

When we change a script that is on the repository we do:

git add with file name 
git commit with file name
git push 

